# -*- coding: utf-8 -*-
"""RL_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZY-TwcevG9YeMVDjAzDg9hczzMmsHkY

IMPORTANT NOTE : PLEASE MAKE SURE THAT THE DATASET IS UPLOADED. IN THE ZIP FILE UPLOAD THE CSV FILE NAMED DATA


> Indented block

**By : Mazen Marei 46-4030 and Kyrillos Sameh 46-7671 **

Importing Libraries
"""

import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd

"""Initializing Variables and importing Dataset"""

alpha = 0.0055 # Learning Rate
dataset = pd.read_csv('Data.csv') #Read CSV file
X = dataset.iloc[:,:-1].values # Extract first col (independant vars)
Y = dataset.iloc[:,-1].values # Extract the last col (dependant var to be predicted)
w = np.random.random((1,1)) # Weight Matrix 
b = np.random.random((1,1)) # Bias Matrix
#w = w.transpose()

#print(w)
#print(b)
print(X)
print(len(X))
#print(len(w))

"""Splitting the Dataset into 20% test and 80% training"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)

print(X_train)
#print(X_train[0,:])
#print(X_train[1][0])

"""**Linear regression Learn Function** :
*based on the pseudo code and equations provided in the Quiz*
"""

def learn(w,b,x,y,alpha):
  N = len(x)
  yh = np.zeros(len(y))
  dw=0
  db=0
  for E in range(3): # for E itterations (Epochs)
    for i in range(N): # for each coordinate in the training set 
      yh[i]=w*x[i]+b # Make predictions using intial w and b
      dw +=  (1/N)*( (-2*x[i])*(y[i]-(w*x[i]+b))) # Calculate the graident of w (change in w) for each x 
      db +=  (1/N)*( -2*(y[i]-(w*x[i]+b))) # Calculate the graident of b (change in b) for each x 
    nw = w-alpha*dw    # Update weight vector  
    nb = b-alpha*db    # Update bias vector 
  return nw,nb # Updated weight and bias vectors

"""**Linear Regression Predict Function** : *based on the pseudo code and equations provided in the Quiz*

"""

def predict(nw,nb,x,y):
  N = len(x)
  yh = np.zeros(len(y))
  for E in range(3): # for E itterations (Epochs)
    for i in range(N): # for each coordinate in the test set
       yh[i]=nw*x[i]+nb # 
  return yh

"""**Linear Regression Main Method**"""

if __name__ == '__main__':
  nw,nb=learn(w,b,X_train,Y_train,alpha) # Make the model learn based on the crearted fn using training set
  prd=predict(nw,nb,X_test,Y_test) # Make some predictions based on new data (test set)

print(prd)
print(nw)
print(nb)
print(len(prd))
print(len(X_train))

"""**Linear Regression Using Scikit-Learn**"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression() # Creates the model
regressor.fit(X_train,Y_train) # Trains the model

Y_pred = regressor.predict(X_test)
print(Y_pred)
print(Y_test)

"""**Tennsor Flow Simple Neural Network **"""

import tensorflow as tf
#Keras makes tf simpler 
#Sequential : makes a sequential model
#1 neuron input , 2 neuron hidden , 1 neuron output
#Dense fully connected neurons for the hidden layer
#model = tf.keras.Sequential([tf.keras.layers.Dense(5,input_dim=1,activation="ReLU"),tf.keras.layers.Dense(1,activation="ReLU")])
model = tf.keras.models.Sequential()
model.add( tf.keras.layers.Dense(1,input_shape=(1,),activation="ReLU"))

#compile takes in optimizer arg : ex: stochastic gradient or adam etc 
# Also takes in loss fn type MSE for our case 
# using Stochastic Gradient Descent Opt.
#opt=tf.keras.optimizers.SGD(learning_rate=0.05) # Calls Stochastic Gradient Descent fn opt=tf.keras.optimizers.SGD(learning_rate=alpha) 
model.compile(optimizer='SGD',loss='MSE') # MSE : mean square error tf.keras.losses.mean_absolute_error

"""Training the NN"""

model.fit(X_train,Y_train,epochs=100,verbose=0) # Trains the model on training sets model.fit(X_train,Y_train,epochs=100)

"""Generating Predictions from the trained NN model"""

prd_nn= model.predict(X_test) # Create predictions based on the test set

print(prd_nn)
print(Y_test)

"""Closed form Solution Multivariate Least Squares Method (LMS)"""

Nt=len(X_train)
x_sum = X_train.sum()
y_sum = Y_train.sum() 
x_sum_sq=0
xy_sum = 0 

for i in range(Nt): 
  x_sum_sq=x_sum_sq+(X_train[i])**2
  xy_sum=xy_sum+(X_train[i])*(Y_train[i])


mat = np.array(([Nt,x_sum],[x_sum,x_sum_sq[0]]))
mat_inv = np.linalg.inv(mat)
vec = np.array([[y_sum],[xy_sum[0]]])
b,w= np.matmul(mat_inv,vec) 
LMS_pred=w*X_test + b

#rint(x_sum_sq)
#print(x_sum)
#print(Nt)
#print(xy_sum)
print(LMS_pred)
print(len(LMS_pred))

"""Plots :"""

plt.scatter(X_test,Y_test,color='red')
plt.plot(X_test,prd,color='blue')
#plt.plot(X_test,nw*X_test+nb,color='blue')
plt.title('Salary vs Experience (Test Set) Implemented Fn')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

"""Plot of the created functions  (prd)

scikit learn (Y_pred)
"""

plt.scatter(X_test,Y_test,color='red')
plt.plot(X_test,Y_pred,color='blue')
plt.title('Salary vs Experience (Test Set) scikit learn')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

"""SNN (prd_nn)"""

plt.scatter(X_test,Y_test,color='red')
plt.plot(X_test,prd_nn,color='blue')
plt.title('Salary vs Experience (Test Set) snn')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

"""LMS (LMS_pred)"""

plt.scatter(X_test,Y_test,color='red')
plt.plot(X_test,LMS_pred,color='blue')
plt.title('Salary vs Experience (Test Set) LMS')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()